services:
  ray_server_inference:
    build: .
    container_name: inference_serve
    shm_size: "2gb"
    environment:
      - RAY_memory_usage_threshold=0.95
    ports:
      - "8001:8001"
    deploy:
      resources:
        limits:
          memory: 4g