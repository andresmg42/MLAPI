services:
    ray_server_inference:
    build: ./
    container_name: inference_serve
    shm_size: "2gb"
    environment:
      - RAY_memory_usage_threshold=0.95
      - RAY_memory_monitor_refresh_ms=0
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
    ports:
      - "8001:8001"
    deploy:
      resources:
        limits:
          memory: 4g
    depends_on:
      - ray_server_train 